{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7d0c777-b05b-4da8-a9d5-8976a2b11599",
   "metadata": {},
   "source": [
    "# subset training data\n",
    "This notebook contains functions to generate subsets from multiple raster sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fdce8e6-4c17-4933-928d-ca681584adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rasterio as rio\n",
    "import rioxarray as rxr\n",
    "import geopandas as gpd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from rioxarray import merge\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5ede579-055f-403e-8a30-4261ed912d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_all_data(aso_path, home_path):\n",
    "    aso_fn = aso_path.split('/')[-1][:-4]\n",
    "    S1_snowon_path = glob(f'{home_path}/data/S1_rtc/S1_snow-on_*_for_{aso_fn}.nc')[0]\n",
    "    S1_snowoff_path = glob(f'{home_path}/data/S1_rtc/S1_snow-off_*_for_{aso_fn}.nc')[0]\n",
    "    S2_path = glob(f'{home_path}/data/S2/S2_*_for_{aso_fn}.nc')[0]\n",
    "    fcf_path = glob(f'{home_path}/data/fcf/fcf_for_{aso_fn}.nc')[0]\n",
    "    dem_path = glob(f'{home_path}/data/cop30/cop30_for_{aso_fn}.nc')[0]\n",
    "    \n",
    "    aso_ds = xr.open_dataset(aso_path).squeeze()\n",
    "    aso_ds = aso_ds.rename({'band_data': 'aso_sd'})\n",
    "    aso_ds['aso_sd'] = aso_ds['aso_sd'].where(aso_ds['aso_sd'] >= 0)\n",
    "    \n",
    "    S1_snowon_ds = xr.open_dataset(S1_snowon_path).squeeze()\n",
    "    S1_snowon_ds = S1_snowon_ds.rename({'vv': 'snowon_vv', 'vh':'snowon_vh'})\n",
    "    S1_snowon_ds = S1_snowon_ds.rio.reproject_match(aso_ds, crs=aso_ds.rio.crs)\n",
    "    S1_snowoff_ds = xr.open_dataset(S1_snowoff_path).squeeze()\n",
    "    S1_snowoff_ds = S1_snowoff_ds.rename({'vv': 'snowoff_vv', 'vh':'snowoff_vh'})\n",
    "    S1_snowoff_ds = S1_snowoff_ds.rio.reproject_match(aso_ds, crs=aso_ds.rio.crs)\n",
    "    S2_ds = xr.open_dataset(S2_path).squeeze()\n",
    "    S2_ds = S2_ds.rio.write_crs(aso_ds.rio.crs)\n",
    "    S2_ds = S2_ds.rio.reproject_match(aso_ds, crs=aso_ds.rio.crs)\n",
    "    fcf_ds = xr.open_dataset(fcf_path).squeeze()\n",
    "    fcf_ds = fcf_ds.rename({'__xarray_dataarray_variable__': 'fcf'})\n",
    "    fcf_ds = fcf_ds.rio.reproject_match(aso_ds, crs=aso_ds.rio.crs)\n",
    "    dem_ds = xr.open_dataset(dem_path).squeeze()\n",
    "    dem_ds = dem_ds.rio.write_crs(aso_ds.rio.crs)\n",
    "    dem_ds = dem_ds.rename({'__xarray_dataarray_variable__': 'elevation'})\n",
    "    dem_ds = dem_ds.rio.reproject_match(aso_ds, crs=aso_ds.rio.crs)\n",
    "    \n",
    "    ds_list = [aso_ds, S1_snowon_ds, S1_snowoff_ds, S2_ds, fcf_ds, dem_ds]\n",
    "    \n",
    "    ds = xr.merge(ds_list, compat='override', join='override').squeeze()\n",
    "\n",
    "    return aso_fn, ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4169ff5c-e968-46f1-a766-b06f736272f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ds(ds, subset_size):\n",
    "    minx = 0\n",
    "    miny = 0\n",
    "    maxx = len(ds.x)-subset_size\n",
    "    maxy = len(ds.y)-subset_size\n",
    "\n",
    "    sub_minx = random.randint(minx, maxx)\n",
    "    sub_miny = random.randint(miny, maxy)\n",
    "    subset = ds.isel(x=slice(sub_minx, sub_minx+subset_size), y=slice(sub_miny, sub_miny+subset_size))\n",
    "    \n",
    "    return subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22185424-4b88-45d5-8d81-6412d690571f",
   "metadata": {},
   "source": [
    "## to do:\n",
    "- redo with only one set of tiles in a global projection and match projection to raster before subsetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4eb4e962-0a5c-48bd-a3df-590076bef411",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on utm10n\n",
      "working on train_utm10n_25km.shp\n",
      "working on ASO_50M_SD_American_20230131_clean\n",
      "total subsets from ASO_50M_SD_American_20230131_clean: 171\n",
      "working on ASO_50M_SD_American_20230413_clean\n",
      "total subsets from ASO_50M_SD_American_20230413_clean: 172\n",
      "working on ASO_50M_SD_American_20230428_clean\n",
      "total subsets from ASO_50M_SD_American_20230428_clean: 95\n",
      "working on ASO_50M_SD_American_20230602_clean\n",
      "total subsets from ASO_50M_SD_American_20230602_clean: 172\n",
      "working on ASO_50M_SD_Feather_20220310_clean\n",
      "total subsets from ASO_50M_SD_Feather_20220310_clean: 521\n",
      "working on ASO_50M_SD_Feather_20220331_clean\n",
      "total subsets from ASO_50M_SD_Feather_20220331_clean: 521\n",
      "working on ASO_50M_SD_Feather_20220429_clean\n",
      "total subsets from ASO_50M_SD_Feather_20220429_clean: 521\n",
      "working on ASO_50M_SD_Feather_20230206_clean\n",
      "total subsets from ASO_50M_SD_Feather_20230206_clean: 485\n",
      "working on ASO_50M_SD_Feather_20230409_clean\n",
      "total subsets from ASO_50M_SD_Feather_20230409_clean: 409\n",
      "working on ASO_50M_SD_Feather_20230428_clean\n",
      "total subsets from ASO_50M_SD_Feather_20230428_clean: 269\n",
      "working on ASO_50M_SD_Feather_20230618_clean\n",
      "total subsets from ASO_50M_SD_Feather_20230618_clean: 514\n",
      "working on ASO_50M_SD_LowerPit_20230212_clean\n",
      "total subsets from ASO_50M_SD_LowerPit_20230212_clean: 182\n",
      "working on ASO_50M_SD_LowerPit_20230425_clean\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m subset_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m subset_count \u001b[38;5;241m<\u001b[39m subset_goal:\n\u001b[0;32m---> 43\u001b[0m     subset_ds \u001b[38;5;241m=\u001b[39m \u001b[43msample_ds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtile_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# check if subset has valid ASO pixels\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39minvert(np\u001b[38;5;241m.\u001b[39misnan(subset_ds\u001b[38;5;241m.\u001b[39maso_sd))\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m, in \u001b[0;36msample_ds\u001b[0;34m(ds, subset_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m sub_minx \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(minx, maxx)\n\u001b[1;32m      8\u001b[0m sub_miny \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(miny, maxy)\n\u001b[0;32m----> 9\u001b[0m subset \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msub_minx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_minx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43msubset_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msub_miny\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_miny\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43msubset_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m subset\n",
      "File \u001b[0;32m~/sw/miniconda3/envs/gda/lib/python3.8/site-packages/xarray/core/dataset.py:2453\u001b[0m, in \u001b[0;36mDataset.isel\u001b[0;34m(self, indexers, drop, missing_dims, **indexers_kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m     variables[name] \u001b[38;5;241m=\u001b[39m var\n\u001b[1;32m   2451\u001b[0m     dims\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mzip\u001b[39m(var\u001b[38;5;241m.\u001b[39mdims, var\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct_direct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoord_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoord_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_close\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2461\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sw/miniconda3/envs/gda/lib/python3.8/site-packages/xarray/core/dataset.py:974\u001b[0m, in \u001b[0;36mDataset._construct_direct\u001b[0;34m(cls, variables, coord_names, dims, attrs, indexes, encoding, close)\u001b[0m\n\u001b[1;32m    972\u001b[0m obj\u001b[38;5;241m.\u001b[39m_variables \u001b[38;5;241m=\u001b[39m variables\n\u001b[1;32m    973\u001b[0m obj\u001b[38;5;241m.\u001b[39m_coord_names \u001b[38;5;241m=\u001b[39m coord_names\n\u001b[0;32m--> 974\u001b[0m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dims\u001b[49m \u001b[38;5;241m=\u001b[39m dims\n\u001b[1;32m    975\u001b[0m obj\u001b[38;5;241m.\u001b[39m_indexes \u001b[38;5;241m=\u001b[39m indexes\n\u001b[1;32m    976\u001b[0m obj\u001b[38;5;241m.\u001b[39m_attrs \u001b[38;5;241m=\u001b[39m attrs\n",
      "File \u001b[0;32m~/sw/miniconda3/envs/gda/lib/python3.8/site-packages/xarray/core/common.py:306\u001b[0m, in \u001b[0;36mAttrAccessMixin.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Objects with ``__slots__`` raise AttributeError if you try setting an\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03mundeclared attribute. This is desirable, but the error message could use some\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03mimprovement.\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 306\u001b[0m     \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;66;03m# Don't accidentally shadow custom AttributeErrors, e.g.\u001b[39;00m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;66;03m# DataArray.dims.setter\u001b[39;00m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name\n\u001b[1;32m    312\u001b[0m     ):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "subset_size=128\n",
    "avg_subs_per_pixel=3\n",
    "\n",
    "home_path = '..'\n",
    "utm_zones = ['utm10n', 'utm11n', 'utm12n', 'utm13n']\n",
    "total_subsets = 0\n",
    "\n",
    "# loop through utm zones\n",
    "for utm_zone in utm_zones:\n",
    "    print(f'working on {utm_zone}')\n",
    "    aso_paths = glob(f'{home_path}/data/ASO/ASO_50m_SD_withS1overpass/{utm_zone}/*')\n",
    "    tile_names = [f'train_{utm_zone}_25km.shp', f'test_{utm_zone}_25km.shp', f'val_{utm_zone}_25km.shp']\n",
    "\n",
    "    #loop through train, val, test tiles\n",
    "    for tile_set in tile_names:\n",
    "        # open tiles \n",
    "        print(f'working on {tile_set}')\n",
    "        tiles = gpd.read_file(f'{home_path}/data/polygons/{tile_set}')\n",
    "\n",
    "        #loop through ASO rasters\n",
    "        for aso_path in aso_paths:\n",
    "            # open aso raster\n",
    "            raster_subsets = 0\n",
    "            aso_fn, ds = open_all_data(aso_path, '..')\n",
    "            print(f'working on {aso_fn}')\n",
    "\n",
    "            # loop through tiles\n",
    "            for tile in tiles.iterrows():\n",
    "                # clip to tile extent\n",
    "                try:\n",
    "                    tile_ds = ds.rio.clip([tile[1].geometry], crs=ds.rio.crs, drop=True)\n",
    "                except: #except if tile does not overlap interferogram\n",
    "                    continue\n",
    "                # set number of subsets to grab based on valid pixel count in tile \n",
    "                tile_pixel_count = np.invert(np.isnan(tile_ds.aso_sd.values)).sum()\n",
    "                subset_goal = round(tile_pixel_count/(subset_size**2)*avg_subs_per_pixel)\n",
    "\n",
    "                # pad ds to tile extent\n",
    "                tile_ds = tile_ds.rio.pad_box(miny=tile[1].bottom, minx=tile[1].left, maxy=tile[1].top, maxx=tile[1].right)\n",
    "\n",
    "                subset_count = 0\n",
    "                while subset_count < subset_goal:\n",
    "                    subset_ds = sample_ds(tile_ds, subset_size)\n",
    "                    # check if subset has valid ASO pixels\n",
    "                    if np.invert(np.isnan(subset_ds.aso_sd)).sum() == 0:\n",
    "                        continue\n",
    "\n",
    "                    # check for valid radar coverage \n",
    "                    elif np.sum(np.invert(np.isnan(subset_ds.aso_sd)) > np.invert(np.isnan(subset_ds.snowon_vv))) > 0:\n",
    "                        continue\n",
    "                    elif np.sum(np.invert(np.isnan(subset_ds.aso_sd)) > np.invert(np.isnan(subset_ds.snowon_vh))) > 0:\n",
    "                        continue\n",
    "                    elif np.sum(np.invert(np.isnan(subset_ds.aso_sd)) > np.invert(np.isnan(subset_ds.snowoff_vv))) > 0:\n",
    "                        continue\n",
    "                    elif np.sum(np.invert(np.isnan(subset_ds.aso_sd)) > np.invert(np.isnan(subset_ds.snowoff_vh))) > 0:\n",
    "                        continue\n",
    "                    \n",
    "                    else: # save subset\n",
    "                        # create map of gaps\n",
    "                        ds['gap_map'] = np.multiply(np.isnan(subset_ds.aso_sd), 1)\n",
    "                        # fill nans with 0 \n",
    "                        subset_ds = subset_ds.fillna(0)\n",
    "                        subset_count+=1\n",
    "                        total_subsets+=1\n",
    "                        raster_subsets+=1\n",
    "                        # subset_ds.to_netcdf(f'{home_path}/data/subsets/{tile_set.split(\"_\")[0]}_subsetsv0/{aso_fn}_tile{int(tile[1].id)}_s{subset_count}.nc')\n",
    "                        # subset_ds.aso_sd.rio.to_raster(f'{home_path}/data/subsets_tif/{tile_set.split(\"_\")[0]}_subsetsv0/{aso_fn}_tile{int(tile[1].id)}_s{subset_count}.tif')\n",
    "                        \n",
    "            print(f'total subsets from {aso_fn}: {raster_subsets}')\n",
    "            gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

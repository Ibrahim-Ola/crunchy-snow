{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7d0c777-b05b-4da8-a9d5-8976a2b11599",
   "metadata": {},
   "source": [
    "# subset training data\n",
    "This notebook contains functions to generate subsets from multiple raster sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb4e962-0a5c-48bd-a3df-590076bef411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_ASO(subset_size=128, \n",
    "               max_time_s=1,\n",
    "               max_per_tile=3):\n",
    "    \"\"\"\n",
    "    Get random samples from multiple ASO rasters with different extents and timestamps\n",
    "    \"\"\"\n",
    "    home_path = '..'\n",
    "    # set number of subsets to 0\n",
    "    subset_counter = 0\n",
    "    granules_sampled = []\n",
    "    tiles = gpd.read_file(f'{home_path}/data/polygons/{subset_type}_RGI_grid_25km.shp')\n",
    "\n",
    "    # loop through tiles\n",
    "    # loop through ASO rasters\n",
    "    # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39166447-a5f7-47fd-a1fc-ad685b145fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from other project: \n",
    "def subset_noise(orbit_list, \n",
    "                 frame_list, \n",
    "                 year_list, \n",
    "                 subsets_desired, \n",
    "                 subset_type,\n",
    "                 subset_size=128, \n",
    "                 max_time_s=1,\n",
    "                 max_per_tile=3):\n",
    "    '''\n",
    "    subset hyp3 outputs using tiles\n",
    "    '''\n",
    "    \n",
    "    home_path = '/mnt/d/indennt'\n",
    "    # set number of subsets to 0\n",
    "    subset_counter = 0\n",
    "    granules_sampled = []\n",
    "    tiles = gpd.read_file(f'{home_path}/polygons/{subset_type}_RGI_grid_25km.shp')\n",
    "    \n",
    "    #with open('granules_sampled.pkl', 'rb') as f:\n",
    "        #granules_sampled = pickle.load(f)\n",
    "    \n",
    "    # continue to run until desired subset number is reached\n",
    "    #while subset_counter < subsets_desired:\n",
    "    for orbit in orbit_list:\n",
    "        signal_path = f'{home_path}/signal_maps/{orbit}'\n",
    "        #random.shuffle(frame_list)\n",
    "        for frame in frame_list:\n",
    "            #random.shuffle(year_list)\n",
    "            for year in year_list:\n",
    "                data_path = f'{home_path}/hyp3/{orbit}/{frame}/{year}'\n",
    "                granule_list = glob(f'{data_path}/*P012*/')\n",
    "                \n",
    "                # loop through noise maps\n",
    "                #random.shuffle(granule_list)\n",
    "                for granule_path in granule_list:\n",
    "                    granule = os.path.basename(os.path.normpath(granule_path))\n",
    "                    \n",
    "                    if granule in granules_sampled:\n",
    "                        print(f'granule {granule} already sampled')\n",
    "                        continue \n",
    "                    \n",
    "                    print(f'working on {orbit}, {frame}, {year}, {granule}')\n",
    "                    granule_counter=0\n",
    "                    \n",
    "                    ds = hyp3_to_xarray_single(granule_path)\n",
    "                    \n",
    "                    signal_ds = xr.open_dataset(f'{signal_path}/{orbit}_mean_signal_masked.tif', cache=False)\n",
    "                    corr_ds = xr.open_dataset(f'{signal_path}/{orbit}_mean_corr.tif', cache=False)\n",
    "                    signal_ds = signal_ds.rio.clip_box(minx=ds.x.min(), miny=ds.y.min(), maxx=ds.x.max(), maxy=ds.y.max())\n",
    "                    corr_ds = corr_ds.rio.clip_box(minx=ds.x.min(), miny=ds.y.min(), maxx=ds.x.max(), maxy=ds.y.max())\n",
    "                    signal_ds = signal_ds.rio.reproject_match(ds.unw_phase, nodata=np.nan).squeeze()\n",
    "                    corr_ds = corr_ds.rio.reproject_match(ds.unw_phase, nodata=np.nan).squeeze()\n",
    "\n",
    "                    ds['signal'] = (('y', 'x'), signal_ds.band_data.values)\n",
    "                    ds['signal_corr'] = (('y', 'x'), corr_ds.band_data.values)\n",
    "        \n",
    "                    # loop through tiles\n",
    "                    tiles = tiles.sample(frac=1)\n",
    "                    for i, tile in tiles.iterrows():\n",
    "                        tile_counter = 0\n",
    "                        \n",
    "                        # clip to tile extent\n",
    "                        try:\n",
    "                            tile_ds = ds.rio.clip([tiles.iloc[i].geometry], crs=ds.rio.crs, drop=True)\n",
    "                        except: #except if tile does not overlap interferogram\n",
    "                            #print(f'no valid subsets in tile {i}')\n",
    "                            continue\n",
    "                        else: #check if valid subset exists in tile\n",
    "                            if np.invert(np.isnan(tile_ds.unw_phase.values)).sum() < subset_size**2:\n",
    "                                #print(f'no valid subsets in tile {i}')\n",
    "                                continue\n",
    "                            else:\n",
    "                                timeout = time.time() + max_time_s # set time to spend on each tile\n",
    "                                # try to find appropriate subsets for a while\n",
    "                                while time.time() < timeout:\n",
    "                                    #grab random subset within sample \n",
    "                                    subset_ds = sample_ds(tile_ds, subset_size)\n",
    "                                    \n",
    "                                    # test if subset elevation is above treeline\n",
    "                                    if np.median(subset_ds.elevation.values) >= 3300:\n",
    "                                        if (subset_ds.signal_corr > 0.85).sum() >= 100:\n",
    "                                            # interpolate small gaps\n",
    "                                            unw_phase_ds = subset_ds.unw_phase.interpolate_na(dim='x', use_coordinate=False)\n",
    "                                            unw_phase_ds = unw_phase_ds.interpolate_na(dim='y', use_coordinate=False)\n",
    "                                            \n",
    "                                            # murp also has gaps to be interpolated, the rest do not\n",
    "                                            murp_phase_ds = subset_ds.murp_phase.interpolate_na(dim='x', use_coordinate=False)\n",
    "                                            murp_phase_ds = murp_phase_ds.interpolate_na(dim='y', use_coordinate=False)\n",
    "        \n",
    "                                            # check if data gaps remain in subset\n",
    "                                            nan_count = (np.isnan(subset_ds.elevation.values).sum() + \n",
    "                                                         np.isnan(subset_ds.era5_phase.values).sum() +\n",
    "                                                         np.isnan(murp_phase_ds.values).sum() +\n",
    "                                                         np.isnan(subset_ds.signal.values).sum() +\n",
    "                                                         np.isnan(unw_phase_ds.values).sum())\n",
    "            \n",
    "                                            if nan_count == 0:\n",
    "                                                subset_counter+=1\n",
    "                                                tile_counter+=1\n",
    "                                                granule_counter+=1\n",
    "                                                subset_name = f'tile{i}_{orbit}_{ds.granule.item()[5:13]}_{ds.granule.item()[21:29]}_sub{subset_counter}.tif'\n",
    "        \n",
    "                                                # calculate era5 and murp noise\n",
    "                                                murp_noise = unw_phase_ds - murp_phase_ds\n",
    "                                                era5_noise = unw_phase_ds - subset_ds.era5_phase\n",
    "        \n",
    "                                                # center signal on 0 (effective local reference point)\n",
    "                                                #subset_ds['signal'] = subset_ds['signal'] - subset_ds['signal'].median(dim=['x', 'y'])\n",
    "\n",
    "                                                # murp to correct signal maps\n",
    "                                                subset_ds = MuRP(subset_ds)\n",
    "                                                \n",
    "                                                # save subset\n",
    "                                                unw_phase_ds.rio.to_raster(f'{home_path}/{subset_type}_subsets/noise/{subset_name}')\n",
    "                                                murp_noise.rio.to_raster(f'{home_path}/{subset_type}_subsets/murp/{subset_name}')\n",
    "                                                era5_noise.rio.to_raster(f'{home_path}/{subset_type}_subsets/era5/{subset_name}')\n",
    "                                                subset_ds.elevation.rio.to_raster(f'{home_path}/{subset_type}_subsets/dem/{subset_name}')\n",
    "                                                subset_ds.signal_MuRP.rio.to_raster(f'{home_path}/{subset_type}_subsets/signal/{subset_name}')\n",
    "                                                #if subset_counter >= subsets_desired:\n",
    "                                                    #print('desired number of subsets reached, exiting')\n",
    "                                                    #return\n",
    "                                                if tile_counter >= max_per_tile:\n",
    "                                                    break\n",
    "                        #print(f'tile {i} subsets: {tile_counter}')\n",
    "                        tile_ds.close()\n",
    "\n",
    "                    print(f'{ds.granule.item()} subsets: {granule_counter}')\n",
    "                    gc.collect()\n",
    "                    granules_sampled.append(granule)\n",
    "                    \n",
    "                    #save list of granules sampled\n",
    "                    with open('granules_sampled.pkl', 'wb') as f:\n",
    "                        pickle.dump(granules_sampled, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
